{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scientific Programming I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<OL>\n",
    "<LI> <A HREF=\"http://scipy-lectures.github.io\">Python Scientific Lecture Notes</A>\n",
    "<LI> <A HREF=\"http://scipy-central.org\">SciPy Central (code snippets, modules, etc.) </A>\n",
    "<LI> <A HREF=\"http://docs.scipy.org/doc/scipy/reference\">SciPy Reference Guide</A>\n",
    "</OL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a SciPy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scipy package contains various toolboxes dedicated to common issues in scientific computing. Its different submodules correspond to different applications, such as interpolation, integration, optimization, image processing, statistics, special functions, etc.\n",
    "\n",
    "scipy can be compared to other standard scientific-computing libraries, such as the GSL (GNU Scientific Library for C and C++), or Matlab’s toolboxes. scipy is the core package for scientific routines in Python; it is meant to operate efficiently on numpy arrays, so that numpy and scipy work hand in hand.\n",
    "\n",
    "Before implementing a routine, it is worth checking if the desired data processing is not already implemented in Scipy. As non-professional programmers, scientists often tend to re-invent the wheel, which leads to buggy, non-optimal, difficult-to-share and unmaintainable code. By contrast, Scipy‘s routines are optimized and tested, and should therefore be used when possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following packages are available in SciPy:\n",
    "\n",
    "<table>\n",
    "<tr> <td> <b> Subpackage </b></td> <td><b> Description </b></td></tr>\n",
    "<tr> <td>cluster</td> <td> \tClustering algorithms</td> </tr>\n",
    "<tr> <td>constants</td> <td> \tPhysical and mathematical constants</td> </tr>\n",
    "<tr> <td>fftpack</td> <td> \tFast Fourier Transform routines</td> </tr>\n",
    "<tr> <td>integrate</td> <td> \tIntegration and ordinary differential equation solvers</td> </tr>\n",
    "<tr> <td>interpolate</td> <td> \tInterpolation and smoothing splines</td> </tr>\n",
    "<tr> <td>io</td> <td> \tInput and Output</td> </tr>\n",
    "<tr> <td>linalg</td> <td> \tLinear algebra</td> </tr>\n",
    "<tr> <td>ndimage</td> <td> \tN-dimensional image processing</td> </tr>\n",
    "<tr> <td>odr</td> <td> \tOrthogonal distance regression</td> </tr>\n",
    "<tr> <td>optimize</td> <td> \tOptimization and root-finding routines</td> </tr>\n",
    "<tr> <td>signal</td> <td> \tSignal processing</td> </tr>\n",
    "<tr> <td>sparse</td> <td> \tSparse matrices and associated routines</td> </tr>\n",
    "<tr> <td>spatial</td> <td> \tSpatial data structures and algorithms</td> </tr>\n",
    "<tr> <td>special</td> <td> \tSpecial functions</td> </tr>\n",
    "<tr> <td>stats</td> <td> \tStatistical distributions and functions</td> </tr>\n",
    "<tr> <td>weave</td> <td> \tC/C++ integration</td> </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "# Increase default figure resolution\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input / Output (scipy.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scipy has built-in functions to read and write to a wide variety of data formats, including Matlab, IDL, and netcdf.  Plain text and binary capabilites are available from numpy.  hdf5 has its own python module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import io\n",
    "a = np.ones((3, 3))\n",
    "io.savemat('file.mat', {'a': a}); # savemat expects a dictionary\n",
    "data = io.loadmat('file.mat', struct_as_record=True)\n",
    "data['a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration (scipy.integrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%latex\n",
    "We want to compute the integral:\n",
    "\n",
    "$$ I = \\int_{x=\\pi}^{2\\pi}\\int_{y=0}^{\\pi}{(y\\sin x + x\\cos y)dydx} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def integrand(y, x):\n",
    "    'y must be the first argument, and x the second.'\n",
    "    return y * np.sin(x) + x * np.cos(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ans, err = integrate.dblquad(integrand, \n",
    "                             np.pi, 2*np.pi,  # x limits\n",
    "                   lambda x: 0,               # y limits\n",
    "                   lambda x: np.pi)\n",
    "print ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Algebra Operations (scipy.linalg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scipy.linalg` contains all the functions in `numpy.linalg` plus some other more advanced ones not contained in `numpy.linalg`.\n",
    "\n",
    "Another advantage of using `scipy.linalg` over `numpy.linalg` is that it is always compiled with BLAS/LAPACK support, while for numpy this is optional. Therefore, the `scipy` version might be faster depending on how `numpy` was installed.\n",
    "\n",
    "Therefore, unless you don’t want to add `scipy` as a dependency to your numpy program, use `scipy.linalg` instead of `numpy.linalg`\n",
    "\n",
    "The most commonly used methods are to calculate the determinant of a matrix (`linalg.det`) and to invert a matrix (`linalg.inv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr1 = np.array([[1, 2], [3, 4]]); arr2 = np.array([[3, 2], [6, 4]]); arr3 = np.ones((3, 3))\n",
    "print \"arr 1 = \\n\",arr1\n",
    "print \"arr 2 = \\n\",arr2\n",
    "print \"arr 3 = \\n\",arr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linalg.det(arr1), linalg.det(arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linalg.det(arr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linalg.inv(arr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# np.allclose returns True if two arrays are element-wise equal within a tolerance\n",
    "np.allclose(np.dot(arr1, linalg.inv(arr1)), np.eye(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print arr3\n",
    "linalg.inv(arr3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a matrix has a determinant of zero LinAlg will raise an error. This is the definition of a Singular matrix (one for which an inverse does not exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print arr1\n",
    "linalg.inv(arr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Fourier Transforms (scipy.fftpack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import fftpack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scipy.fftpack module allows to compute fast Fourier transforms. As an illustration, a (noisy) input signal may look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_step = 0.02\n",
    "period = 5.\n",
    "time_vec = np.arange(0, 20, time_step)\n",
    "sig = np.sin(2 * np.pi / period * time_vec) + 0.5 * np.random.randn(time_vec.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(time_vec, sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observer doesn’t know the signal frequency, only the sampling time step of the signal sig. The signal is supposed to come from a real function so the Fourier transform will be symmetric. The scipy.fftpack.fftfreq() function will generate the sampling frequencies and scipy.fftpack.fft() will compute the fast Fourier transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_freq = fftpack.fftfreq(sig.size, d=time_step)\n",
    "sig_fft = fftpack.fft(sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the resulting power is symmetric, only the positive part of the spectrum needs to be used for finding the frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pidxs = np.where(sample_freq > 0)\n",
    "freqs = sample_freq[pidxs]\n",
    "power = np.abs(sig_fft)[pidxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(freqs, power)\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.ylabel('plower')\n",
    "axes = plt.axes([0.3, 0.3, 0.5, 0.5])\n",
    "plt.title('Peak frequency')\n",
    "plt.plot(freqs[:8], power[:8])\n",
    "plt.setp(axes, yticks=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The signal frequency can be found by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq = freqs[power.argmax()]\n",
    "freq, np.allclose(freq, 1./period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the high-frequency noise will be removed from the Fourier transformed signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sig_fft[np.abs(sample_freq) > freq] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting filtered signal can be computed by the scipy.fftpack.ifft() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_sig = fftpack.ifft(sig_fft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result can be viewed with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(time_vec, sig)\n",
    "plt.plot(time_vec, main_sig, linewidth=3)\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Amplitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization and Fitting (scipy.optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scipy.optimize module provides useful algorithms for function minimization (scalar or multi-dimensional), curve fitting and root finding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the minimum of a scalar function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = lambda x: x**2 + 10*np.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.arange(-10, 10, 0.1)\n",
    "plt.plot(x, f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function has a global minimum around -1.3 and a local minimum around 3.8.\n",
    "\n",
    "The general and efficient way to find a minimum for this function is to conduct a gradient descent starting from a given initial point. The BFGS algorithm is a good way of doing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimize.fmin_bfgs(f, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A possible issue with this approach is that, if the function has local minima the algorithm may find these local minima instead of the global minimum depending on the initial point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimize.fmin_bfgs(f, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don’t know the neighborhood of the global minimum to choose the initial point, we need to resort to costlier global optimization. To find the global minimum, the simplest algorithm is the brute force algorithm, in which the function is evaluated on each point of a given grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid = (-10, 10, 0.1)\n",
    "xmin_global = optimize.brute(f, (grid,))\n",
    "xmin_global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For larger grid sizes, scipy.optimize.brute() becomes quite slow. scipy.optimize.anneal() provides an alternative, using simulated annealing. More efficient algorithms for different classes of global optimization problems exist, but this is out of the scope of scipy. Some useful packages for global optimization are OpenOpt, IPOPT, PyGMO and PyEvolve.\n",
    "\n",
    "To find the local minimum, let’s constraint the variable to the interval (0, 10) using scipy.optimize.fminbound():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xmin_local = optimize.fminbound(f, 0, 10)\n",
    "xmin_local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the roots of a scalar function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find a root, i.e. a point where f(x) = 0, of the function f above we can use for example scipy.optimize.fsolve():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root = optimize.fsolve(f, 1)  # our initial guess is 1\n",
    "root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that only one root is found. Inspecting the plot of f reveals that there is a second root around -2.5. We find the exact value of it by adjusting our initial guess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root2 = optimize.fsolve(f, -2.5)\n",
    "root2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curve Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have data sampled from f with some noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xdata = np.linspace(-10, 10, num=20)\n",
    "ydata = f(xdata) + np.random.randn(xdata.size)\n",
    "plt.plot(xdata,ydata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we know the functional form of the function from which the samples were drawn (x^2 + sin(x) in this case) but not the amplitudes of the terms, we can find those by least squares curve fitting. First we have to define the function to fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f2 = lambda x, a, b: a*x**2 + b*np.sin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use scipy.optimize.curve_fit() to find a and b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "guess = [2, 2]\n",
    "params, params_covariance = optimize.curve_fit(f2, xdata, ydata, guess)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, f(x), 'b-', label=\"f(x)\")\n",
    "ax.plot(x, f2(x, *params), 'r--', label=\"Curve fit result\")\n",
    "xmins = np.array([xmin_global[0], xmin_local])\n",
    "ax.plot(xmins, f(xmins), 'go', label=\"Minima\")\n",
    "roots = np.array([root, root2])\n",
    "ax.plot(roots, f(roots), 'kv', label=\"Roots\")\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Note: In Scipy >= 0.11 unified interfaces to all minimization and root finding algorithms are available: scipy.optimize.minimize(), scipy.optimize.minimize_scalar() and scipy.optimize.root(). They allow comparing various algorithms easily through the method keyword.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Curve fitting of temperature data\n",
    "\n",
    "The temperature extremes in Alaska for each month, starting in January, are given by (in degrees Celcius):\n",
    "\n",
    "| Temp | Jan | Feb | Mar | Apr | May | Jun | Jul | Aug | Sep | Oct | Nov | Dec |\n",
    "| ------------- |:-------------:| -----:|\n",
    "| max: | 17 | 19 | 21 | 28 | 33 | 38 | 37 | 37 | 31 | 23 | 19 | 18 |\n",
    "| min: | -62 | -59 | -56 | -46 | -32 | -18 | -9 | -13 | -25 | -46 | -52 | -58 |\n",
    "\n",
    "Plot these temperature extremes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max = [17,  19,  21,  28,  33,  38, 37,  37,  31,  23,  19,  18]\n",
    "min = [-62, -59, -56, -46, -32, -18, -9, -13, -25, -46, -52, -58]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(range(12),max,'r-', label='Max')\n",
    "plt.plot(range(12),min,'b-', label='Min')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Temperature ($^o C$)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that can describe min and max temperatures. Hint: this function has to have a period of 1 year. Hint: include a time offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = lambda t, a, b, c: b*np.sin(2.*math.pi*t+a)+c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit this function to the data with `scipy.optimize.curve_fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "guess = [6, 1, 1]\n",
    "print \"min temp\"\n",
    "params, params_covariance = optimize.curve_fit(temp, range(12), min, guess)\n",
    "print params\n",
    "\n",
    "print \"max temp\"\n",
    "params2, params_covariance2 = optimize.curve_fit(temp, range(12), max, guess)\n",
    "print params2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the result. Is the fit reasonable? If not, why?\n",
    "\n",
    "Is the time offset for min and max temperatures the same within the fit accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x=np.linspace(-1,12,1000)\n",
    "ax.plot(range(12), min, 'b.', label=\"min\")\n",
    "ax.plot(range(12), max, 'r.', label=\"max\")\n",
    "ax.plot(x, temp(x, *params), 'y--', label=\"min fit\")\n",
    "ax.plot(x, temp(x, *params2), 'g--', label=\"max fit\")\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Temperature ($^o C$)')\n",
    "ax.set_xlim=[-1.,11.]\n",
    "ax.set_ylim=[-80,60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation (scipy.interpolate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scipy.interpolate is useful for fitting a function from experimental data and thus evaluating points where no measure exists. The module is based on the FITPACK Fortran subroutines from the netlib project.\n",
    "\n",
    "By imagining experimental data close to a sine function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "measured_time = np.linspace(0, 1, 10)\n",
    "noise = (np.random.random(10)*2 - 1) * 1e-1\n",
    "measures = np.sin(2 * np.pi * measured_time) + noise\n",
    "plt.plot(measured_time,measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scipy.interpolate.interp1d class can build a linear interpolation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "linear_interp = interp1d(measured_time, measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the scipy.interpolate.linear_interp instance needs to be evaluated at the time of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "computed_time = np.linspace(0, 1, 50)\n",
    "linear_results = linear_interp(computed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cubic interpolation can also be selected by providing the kind optional keyword argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cubic_interp = interp1d(measured_time, measures, kind='cubic')\n",
    "cubic_results = cubic_interp(computed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(measured_time, measures, 'o', ms=6, label='measures')\n",
    "plt.plot(computed_time, linear_results, label='linear interp')\n",
    "plt.plot(computed_time, cubic_results, label='cubic interp')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics and Random Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module scipy.stats contains statistical tools and probabilistic descriptions of random processes. Random number generators for various random process can be found in numpy.random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram and probability density function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given observations of a random process, their histogram is an estimator of the random process’s PDF (probability density function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.random.normal(size=1000)\n",
    "bins = np.arange(-4, 5)\n",
    "bins\n",
    "\n",
    "histogram = np.histogram(a, bins=bins, normed=True)[0]\n",
    "bins = 0.5*(bins[1:] + bins[:-1])\n",
    "bins\n",
    "\n",
    "from scipy import stats\n",
    "b = stats.norm.pdf(bins)  # norm is a distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(bins, histogram)\n",
    "plt.plot(bins, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we know that the random process belongs to a given family of random processes, such as normal processes, we can do a maximum-likelihood fit of the observations to estimate the parameters of the underlying distribution. Here we fit a normal process to the observed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc, std = stats.norm.fit(a)\n",
    "loc, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percentiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median is the value with half of the observations below, and half above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.median(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also called the percentile 50, because 50% of the observation are below it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats.scoreatpercentile(a, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can calculate the percentile 90:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats.scoreatpercentile(a, 90) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentile is an estimator of the CDF: cumulative distribution function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A statistical test is a decision indicator. For instance, if we have two sets of observations, that we assume are generated from Gaussian processes, we can use a T-test to decide whether the two sets of observations are significantly different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.random.normal(0, 1, size=100)\n",
    "b = np.random.normal(1, 1, size=10)\n",
    "stats.ttest_ind(a, b)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting output is composed of:\n",
    "\n",
    "1) The T statistic value: it is a number the sign of which is proportional to the difference between the two random processes and the magnitude is related to the significance of this difference.\n",
    "\n",
    "2) The p value: the probability of both processes being identical. If it is close to 1, the two process are almost certainly identical. The closer it is to zero, the more likely it is that the processes have different means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal Processing (scipy.signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scipy.signal.detrend(): remove linear trend from signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = np.linspace(0, 5, 100)\n",
    "x = t + np.random.normal(size=100)\n",
    "\n",
    "plt.plot(t, x, linewidth=3)\n",
    "plt.plot(t, signal.detrend(x), linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scipy.signal.resample(): resample a signal to n points using FFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = np.linspace(0, 5, 100)\n",
    "x = np.sin(t)\n",
    "\n",
    "plt.plot(t, x, linewidth=3)\n",
    "plt.plot(t[::2], signal.resample(x, 50), 'ko')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scipy.signal has many window functions: scipy.signal.hamming(), scipy.signal.bartlett(), scipy.signal.blackman()...\n",
    "\n",
    "scipy.signal has filtering (median filter scipy.signal.medfilt(), Wiener scipy.signal.wiener()), but we will discuss this in the image section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Processing (scipy.ndimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image processing routines may be sorted according to the category of processing they perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geometrical transformations on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing orientation, resolution, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import misc\n",
    "ascent = misc.ascent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(ascent, cmap=cm.gray)\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shifted_ascent = ndimage.shift(ascent, (50, 50))\n",
    "shifted_ascent2 = ndimage.shift(ascent, (50, 50), mode='nearest')\n",
    "rotated_ascent = ndimage.rotate(ascent, 30)\n",
    "cropped_ascent = ascent[50:-50, 50:-50]\n",
    "zoomed_ascent = ndimage.zoom(ascent, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(151)\n",
    "ax.imshow(shifted_ascent, cmap=cm.gray)\n",
    "ax.axis('off')\n",
    "ax2 = fig.add_subplot(152)\n",
    "ax2.imshow(shifted_ascent2, cmap=cm.gray)\n",
    "ax2.axis('off')\n",
    "ax3 = fig.add_subplot(153)\n",
    "ax3.imshow(rotated_ascent, cmap=cm.gray)\n",
    "ax3.axis('off')\n",
    "ax4 = fig.add_subplot(154)\n",
    "ax4.imshow(cropped_ascent, cmap=cm.gray)\n",
    "ax4.axis('off')\n",
    "ax5 = fig.add_subplot(155)\n",
    "ax5.imshow(zoomed_ascent, cmap=cm.gray)\n",
    "ax5.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noisy_ascent = np.copy(ascent).astype(np.float)\n",
    "noisy_ascent += ascent.std()*0.5*np.random.standard_normal(ascent.shape)\n",
    "blurred_ascent = ndimage.gaussian_filter(noisy_ascent, sigma=3)\n",
    "median_ascent = ndimage.median_filter(blurred_ascent, size=5)\n",
    "wiener_ascent = signal.wiener(blurred_ascent, (5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(141)\n",
    "ax.imshow(noisy_ascent, cmap=cm.gray)\n",
    "ax.axis('off')\n",
    "ax.set_title(\"noisy ascent\")\n",
    "ax2 = fig.add_subplot(142)\n",
    "ax2.imshow(blurred_ascent, cmap=cm.gray)\n",
    "ax2.axis('off')\n",
    "ax2.set_title(\"Gaussian filter\")\n",
    "ax3 = fig.add_subplot(143)\n",
    "ax3.imshow(median_ascent, cmap=cm.gray)\n",
    "ax3.axis('off')\n",
    "ax3.set_title(\"median filter\")\n",
    "ax4 = fig.add_subplot(144)\n",
    "ax4.imshow(wiener_ascent, cmap=cm.gray)\n",
    "ax4.axis('off')\n",
    "ax4.set_title(\"Weiner filter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measurements on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = np.indices((100, 100))\n",
    "sig = np.sin(2*np.pi*x/50.)*np.sin(2*np.pi*y/50.)*(1+x*y/50.**2)**2\n",
    "mask = sig > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look for various information about the objects in the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels, nb = ndimage.label(mask)\n",
    "nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "areas = ndimage.sum(mask, labels, xrange(1, labels.max()+1))\n",
    "areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxima = ndimage.maximum(sig, labels, xrange(1, labels.max()+1))\n",
    "maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndimage.find_objects(labels==4)\n",
    "sl = ndimage.find_objects(labels==4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(131)\n",
    "ax.imshow(sig)\n",
    "ax.axis('off')\n",
    "ax.set_title(\"sig\")\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.imshow(mask)\n",
    "ax2.axis('off')\n",
    "ax2.set_title(\"mask\")\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.imshow(labels)\n",
    "ax3.axis('off')\n",
    "ax3.set_title(\"labels\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout Session: Imaging Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG src=\"files/MV_HFV_012.jpg\"></IMG>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<OL>\n",
    "<LI> Open the image file MV_HFV_012.png and display it.  This Scanning Element Microscopy image shows a glass sample (light gray) with some bubbles (on black) and unmolten sand grains (dark gray). \n",
    "\n",
    "<LI> Crop the image to remove the lower panel with measure information.\n",
    "\n",
    "<LI> Slightly filter the image with a median filter in order to refine its histogram. Check how the histogram changes.\n",
    "\n",
    "<LI> Using the histogram of the filtered image, determine thresholds that allow to define masks for sand pixels, glass pixels and bubble pixels.\n",
    "\n",
    "<LI> Display an image in which the three phases are colored with three different colors.\n",
    "\n",
    "<LI> Attribute labels to all bubbles and sand grains, and remove from the sand mask grains that are smaller than 10 pixels. To do so, use ndimage.sum or np.bincount to compute the grain sizes.\n",
    "\n",
    "<LI> Compute the mean and median size of bubbles.\n",
    "\n",
    "</OL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
